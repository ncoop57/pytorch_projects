{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBoW(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dims, vocab_size, context_size):\n",
    "        super(CBoW, self).__init__()\n",
    "        self.embeds = nn.Embedding(vocab_size, embed_dims)\n",
    "        \n",
    "        self.l1 = nn.Linear(embed_dims * context_size * 2, 128)\n",
    "        self.l2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        output = self.embeds(inputs).view((1, -1))\n",
    "        output = F.relu(self.l1(output))\n",
    "        output = self.l2(output)\n",
    "        log_probs = F.log_softmax(output, dim = 1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 24\n",
       " 18\n",
       " 34\n",
       " 40\n",
       "[torch.LongTensor of size 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_context_vector(context, word_to_idx):\n",
    "    idxs = [word_to_idx[w] for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "\n",
    "make_context_vector(data[0][0], word_to_idx)  # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "losses = []\n",
    "loss_fn = nn.NLLLoss()\n",
    "cbow_model = CBoW(10, vocab_size, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(cbow_model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " 224.5558\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 222.9171\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 221.2901\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 219.6741\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 218.0686\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 216.4718\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 214.8834\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 213.3023\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 211.7293\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 210.1639\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 208.6043\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 207.0513\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 205.5040\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 203.9620\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 202.4240\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 200.8908\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 199.3607\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 197.8350\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 196.3131\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 194.7925\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 193.2758\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 191.7612\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 190.2483\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 188.7385\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 187.2293\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 185.7206\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 184.2137\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 182.7087\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 181.2045\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 179.7026\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 178.2018\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 176.7016\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 175.2004\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 173.7027\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 172.2055\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 170.7093\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 169.2160\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 167.7209\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 166.2233\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 164.7255\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 163.2272\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 161.7282\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 160.2311\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 158.7333\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 157.2383\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 155.7429\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 154.2484\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 152.7531\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 151.2578\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 149.7626\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 148.2688\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 146.7745\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 145.2803\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 143.7883\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 142.2980\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 140.8070\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 139.3167\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 137.8294\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 136.3418\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 134.8558\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 133.3736\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 131.8921\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 130.4127\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 128.9380\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 127.4626\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 125.9921\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 124.5248\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 123.0629\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 121.6041\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 120.1494\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 118.6964\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 117.2514\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 115.8079\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 114.3700\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 112.9379\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 111.5096\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 110.0899\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 108.6735\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 107.2650\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 105.8613\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 104.4632\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 103.0731\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 101.6900\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 100.3134\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 98.9469\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 97.5852\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 96.2358\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 94.8928\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 93.5610\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 92.2371\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 90.9234\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 89.6179\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 88.3249\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 87.0411\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 85.7660\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 84.5027\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 83.2496\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 82.0077\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 80.7770\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 79.5587\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 78.3521\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 77.1567\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 75.9724\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 74.8017\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 73.6444\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 72.4979\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 71.3675\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 70.2485\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69.1427\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 68.0505\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in data:\n",
    "        context_var = make_context_vector(context, word_to_idx)\n",
    "        cbow_model.zero_grad()\n",
    "        \n",
    "        log_probs = cbow_model(context_var)\n",
    "        loss = loss_fn(log_probs, autograd.Variable(torch.LongTensor([word_to_idx[target]])))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "    \n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.8660\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 29\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# Testing model\n",
    "context = ['idea', 'of', 'process', 'is']\n",
    "context_var = make_context_vector(context, word_to_idx)\n",
    "log_probs = cbow_model(context_var)\n",
    "\n",
    "values, indices = log_probs.max(1)\n",
    "\n",
    "print(values)\n",
    "print(indices)\n",
    "print(list(vocab)[indices.data[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
